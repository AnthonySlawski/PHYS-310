{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ML5YoNrZS_Gc"
   },
   "source": [
    "# Lab Notebook 8 - SVM classification in particle physics\n",
    "\n",
    "In this notebook we will learn about applying SVMs to a larger data set from particle physics.\n",
    "\n",
    "_Data by [Sascha Caron](https://www.nikhef.nl/~scaron/). Modified from the Notebook by Viviana Acquaviva (2023). License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "scz89F7WS_Ge"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, LinearSVC # New algorithm!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV # New! This will be used to explore different hyperparameter choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-V-0pnD9S_Gf"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "rc('text', usetex=False)\n",
    "plt.rc('font', size=15)\n",
    "plt.rc('axes', titlesize=16)\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('legend', fontsize=12)\n",
    "plt.rc('figure', titlesize=16)\n",
    "plt.rc('figure',figsize=(8,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGSpyK91S_Gu"
   },
   "source": [
    "## Step 1\n",
    "\n",
    "Read in features and labels from 'ParticleID_features.csv' and 'ParticleID_labels.txt' using pandas. Read in labels from 'ParticleID_labels.txt'. Explore the data set to get an idea of what it looks like (ex: look at first few rows, shape, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 68 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   ID       5000 non-null   int64  \n",
      " 1   MET      5000 non-null   float64\n",
      " 2   METphi   5000 non-null   float64\n",
      " 3   Type_1   5000 non-null   object \n",
      " 4   P1       5000 non-null   float64\n",
      " 5   P2       5000 non-null   float64\n",
      " 6   P3       5000 non-null   float64\n",
      " 7   P4       5000 non-null   float64\n",
      " 8   Type_2   4997 non-null   object \n",
      " 9   P5       4997 non-null   float64\n",
      " 10  P6       4997 non-null   float64\n",
      " 11  P7       4997 non-null   float64\n",
      " 12  P8       4997 non-null   float64\n",
      " 13  Type_3   4950 non-null   object \n",
      " 14  P9       4950 non-null   float64\n",
      " 15  P10      4950 non-null   float64\n",
      " 16  P11      4950 non-null   float64\n",
      " 17  P12      4950 non-null   float64\n",
      " 18  Type_4   4717 non-null   object \n",
      " 19  P13      4717 non-null   float64\n",
      " 20  P14      4717 non-null   float64\n",
      " 21  P15      4717 non-null   float64\n",
      " 22  P16      4717 non-null   float64\n",
      " 23  Type_5   4002 non-null   object \n",
      " 24  P17      4002 non-null   float64\n",
      " 25  P18      4002 non-null   float64\n",
      " 26  P19      4002 non-null   float64\n",
      " 27  P20      4002 non-null   float64\n",
      " 28  Type_6   2871 non-null   object \n",
      " 29  P21      2871 non-null   float64\n",
      " 30  P22      2871 non-null   float64\n",
      " 31  P23      2871 non-null   float64\n",
      " 32  P24      2871 non-null   float64\n",
      " 33  Type_7   1889 non-null   object \n",
      " 34  P25      1889 non-null   float64\n",
      " 35  P26      1889 non-null   float64\n",
      " 36  P27      1889 non-null   float64\n",
      " 37  P28      1889 non-null   float64\n",
      " 38  Type_8   1186 non-null   object \n",
      " 39  P29      1186 non-null   float64\n",
      " 40  P30      1186 non-null   float64\n",
      " 41  P31      1186 non-null   float64\n",
      " 42  P32      1186 non-null   float64\n",
      " 43  Type_9   729 non-null    object \n",
      " 44  P33      729 non-null    float64\n",
      " 45  P34      729 non-null    float64\n",
      " 46  P35      729 non-null    float64\n",
      " 47  P36      729 non-null    float64\n",
      " 48  Type_10  442 non-null    object \n",
      " 49  P37      442 non-null    float64\n",
      " 50  P38      442 non-null    float64\n",
      " 51  P39      442 non-null    float64\n",
      " 52  P40      442 non-null    float64\n",
      " 53  Type_11  261 non-null    object \n",
      " 54  P41      261 non-null    float64\n",
      " 55  P42      261 non-null    float64\n",
      " 56  P43      261 non-null    float64\n",
      " 57  P44      261 non-null    float64\n",
      " 58  Type_12  127 non-null    object \n",
      " 59  P45      127 non-null    float64\n",
      " 60  P46      127 non-null    float64\n",
      " 61  P47      127 non-null    float64\n",
      " 62  P48      127 non-null    float64\n",
      " 63  Type_13  56 non-null     object \n",
      " 64  P49      56 non-null     float64\n",
      " 65  P50      56 non-null     float64\n",
      " 66  P51      56 non-null     float64\n",
      " 67  P52      56 non-null     float64\n",
      "dtypes: float64(54), int64(1), object(13)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv(\"ParticleID_features.csv\")\n",
    "pd.read_csv(\"ParticleID_features.csv\")\n",
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Xd7WnozpS_Gu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ttbar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ttbar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ttbar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ttbar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ttbar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ttbar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>4top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>ttbar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>ttbar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>ttbar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4999 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ttbar\n",
       "0     ttbar\n",
       "1     ttbar\n",
       "2     ttbar\n",
       "3     ttbar\n",
       "4     ttbar\n",
       "...     ...\n",
       "4994   4top\n",
       "4995  ttbar\n",
       "4996  ttbar\n",
       "4997   4top\n",
       "4998  ttbar\n",
       "\n",
       "[4999 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\"ParticleID_labels.txt\", header=None, names=['Labels'])\n",
    "pd.read_csv(\"ParticleID_labels.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EQH55WhS_Gv"
   },
   "source": [
    "## Step 2\n",
    "\n",
    "The labels are in the form 'ttbar' and '4top'. Turn these categorical (string-type) labels into an array of zeros and ones, where 'ttbar'=0 and '4top'=1 using 'LabelEncoder' from sklearn.preprocessing. Call this new array \"target\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lzB-AaSrS_Gv"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "target= le.fit_transform(labels.values.ravel())\n",
    "target = (target-1)*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTJcgL79S_Gx"
   },
   "source": [
    "## Step 3\n",
    "\n",
    "Using describe() on features, look at the \"count\" row. Some columns only contain fractions of the total number of data set rows, due to the variable number of products in each collision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "asuEVhroS_Gy",
    "outputId": "178caa3d-71fc-4db0-c289-3156a303d355"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MET</th>\n",
       "      <th>METphi</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>P9</th>\n",
       "      <th>P10</th>\n",
       "      <th>P11</th>\n",
       "      <th>P12</th>\n",
       "      <th>P13</th>\n",
       "      <th>P14</th>\n",
       "      <th>P15</th>\n",
       "      <th>P16</th>\n",
       "      <th>P17</th>\n",
       "      <th>P18</th>\n",
       "      <th>P19</th>\n",
       "      <th>P20</th>\n",
       "      <th>P21</th>\n",
       "      <th>P22</th>\n",
       "      <th>P23</th>\n",
       "      <th>P24</th>\n",
       "      <th>P25</th>\n",
       "      <th>P26</th>\n",
       "      <th>P27</th>\n",
       "      <th>P28</th>\n",
       "      <th>P29</th>\n",
       "      <th>P30</th>\n",
       "      <th>P31</th>\n",
       "      <th>P32</th>\n",
       "      <th>P33</th>\n",
       "      <th>P34</th>\n",
       "      <th>P35</th>\n",
       "      <th>P36</th>\n",
       "      <th>P37</th>\n",
       "      <th>P38</th>\n",
       "      <th>P39</th>\n",
       "      <th>P40</th>\n",
       "      <th>P41</th>\n",
       "      <th>P42</th>\n",
       "      <th>P43</th>\n",
       "      <th>P44</th>\n",
       "      <th>P45</th>\n",
       "      <th>P46</th>\n",
       "      <th>P47</th>\n",
       "      <th>P48</th>\n",
       "      <th>P49</th>\n",
       "      <th>P50</th>\n",
       "      <th>P51</th>\n",
       "      <th>P52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>4.997000e+03</td>\n",
       "      <td>4.997000e+03</td>\n",
       "      <td>4997.000000</td>\n",
       "      <td>4997.000000</td>\n",
       "      <td>4.950000e+03</td>\n",
       "      <td>4950.000000</td>\n",
       "      <td>4950.000000</td>\n",
       "      <td>4950.000000</td>\n",
       "      <td>4.717000e+03</td>\n",
       "      <td>4717.000000</td>\n",
       "      <td>4717.000000</td>\n",
       "      <td>4717.000000</td>\n",
       "      <td>4.002000e+03</td>\n",
       "      <td>4002.000000</td>\n",
       "      <td>4002.000000</td>\n",
       "      <td>4002.000000</td>\n",
       "      <td>2.871000e+03</td>\n",
       "      <td>2871.00000</td>\n",
       "      <td>2871.000000</td>\n",
       "      <td>2871.000000</td>\n",
       "      <td>1.889000e+03</td>\n",
       "      <td>1889.000000</td>\n",
       "      <td>1889.000000</td>\n",
       "      <td>1889.000000</td>\n",
       "      <td>1.186000e+03</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>7.290000e+02</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>2.610000e+02</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>1.270000e+02</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>5.600000e+01</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2499.500000</td>\n",
       "      <td>64071.074332</td>\n",
       "      <td>-0.028916</td>\n",
       "      <td>3.301357e+05</td>\n",
       "      <td>1.540486e+05</td>\n",
       "      <td>-0.039812</td>\n",
       "      <td>-0.003049</td>\n",
       "      <td>2.527799e+05</td>\n",
       "      <td>1.080302e+05</td>\n",
       "      <td>-0.029936</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>2.117980e+05</td>\n",
       "      <td>74863.343131</td>\n",
       "      <td>-0.025104</td>\n",
       "      <td>0.011845</td>\n",
       "      <td>1.805997e+05</td>\n",
       "      <td>57289.049481</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>0.045266</td>\n",
       "      <td>1.780366e+05</td>\n",
       "      <td>48798.018516</td>\n",
       "      <td>0.015167</td>\n",
       "      <td>-0.031312</td>\n",
       "      <td>1.705620e+05</td>\n",
       "      <td>44042.67015</td>\n",
       "      <td>-0.022948</td>\n",
       "      <td>0.014522</td>\n",
       "      <td>1.628825e+05</td>\n",
       "      <td>41151.069666</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>1.581409e+05</td>\n",
       "      <td>40250.387015</td>\n",
       "      <td>0.072349</td>\n",
       "      <td>-0.035907</td>\n",
       "      <td>1.596814e+05</td>\n",
       "      <td>40139.289849</td>\n",
       "      <td>0.061654</td>\n",
       "      <td>-0.045868</td>\n",
       "      <td>1.574039e+05</td>\n",
       "      <td>39703.038235</td>\n",
       "      <td>0.118543</td>\n",
       "      <td>0.024249</td>\n",
       "      <td>1.561160e+05</td>\n",
       "      <td>38173.716092</td>\n",
       "      <td>0.029455</td>\n",
       "      <td>0.026422</td>\n",
       "      <td>1.631051e+05</td>\n",
       "      <td>34876.849606</td>\n",
       "      <td>0.206978</td>\n",
       "      <td>-0.001085</td>\n",
       "      <td>1.456600e+05</td>\n",
       "      <td>36151.183929</td>\n",
       "      <td>-0.000879</td>\n",
       "      <td>0.219260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1443.520003</td>\n",
       "      <td>60525.122480</td>\n",
       "      <td>1.819257</td>\n",
       "      <td>3.068202e+05</td>\n",
       "      <td>1.149469e+05</td>\n",
       "      <td>1.361762</td>\n",
       "      <td>1.814855</td>\n",
       "      <td>2.638580e+05</td>\n",
       "      <td>8.136261e+04</td>\n",
       "      <td>1.439105</td>\n",
       "      <td>1.828832</td>\n",
       "      <td>2.510361e+05</td>\n",
       "      <td>46309.512365</td>\n",
       "      <td>1.577316</td>\n",
       "      <td>1.802715</td>\n",
       "      <td>2.383403e+05</td>\n",
       "      <td>32013.857623</td>\n",
       "      <td>1.634072</td>\n",
       "      <td>1.812078</td>\n",
       "      <td>2.577958e+05</td>\n",
       "      <td>26252.978520</td>\n",
       "      <td>1.744489</td>\n",
       "      <td>1.784248</td>\n",
       "      <td>2.381745e+05</td>\n",
       "      <td>23510.65367</td>\n",
       "      <td>1.806611</td>\n",
       "      <td>1.811101</td>\n",
       "      <td>2.269341e+05</td>\n",
       "      <td>20988.953157</td>\n",
       "      <td>1.815312</td>\n",
       "      <td>1.771888</td>\n",
       "      <td>2.118782e+05</td>\n",
       "      <td>26556.025657</td>\n",
       "      <td>1.836492</td>\n",
       "      <td>1.796932</td>\n",
       "      <td>2.308620e+05</td>\n",
       "      <td>30074.756789</td>\n",
       "      <td>1.842798</td>\n",
       "      <td>1.788596</td>\n",
       "      <td>2.165489e+05</td>\n",
       "      <td>30502.312276</td>\n",
       "      <td>1.872084</td>\n",
       "      <td>1.826435</td>\n",
       "      <td>2.319016e+05</td>\n",
       "      <td>29324.658352</td>\n",
       "      <td>1.884750</td>\n",
       "      <td>1.753017</td>\n",
       "      <td>2.248603e+05</td>\n",
       "      <td>20433.767238</td>\n",
       "      <td>1.998859</td>\n",
       "      <td>1.949004</td>\n",
       "      <td>1.943657e+05</td>\n",
       "      <td>25861.883410</td>\n",
       "      <td>1.941707</td>\n",
       "      <td>1.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>290.756000</td>\n",
       "      <td>-3.141010</td>\n",
       "      <td>3.857940e+04</td>\n",
       "      <td>2.825400e+04</td>\n",
       "      <td>-4.110220</td>\n",
       "      <td>-3.140710</td>\n",
       "      <td>1.087540e+04</td>\n",
       "      <td>1.080000e+04</td>\n",
       "      <td>-4.668790</td>\n",
       "      <td>-3.140530</td>\n",
       "      <td>1.221050e+04</td>\n",
       "      <td>10639.800000</td>\n",
       "      <td>-4.520250</td>\n",
       "      <td>-3.141480</td>\n",
       "      <td>1.169190e+04</td>\n",
       "      <td>10818.000000</td>\n",
       "      <td>-4.616550</td>\n",
       "      <td>-3.136130</td>\n",
       "      <td>1.110310e+04</td>\n",
       "      <td>10287.000000</td>\n",
       "      <td>-4.778980</td>\n",
       "      <td>-3.139040</td>\n",
       "      <td>1.070330e+04</td>\n",
       "      <td>10066.90000</td>\n",
       "      <td>-4.930230</td>\n",
       "      <td>-3.140380</td>\n",
       "      <td>1.197700e+04</td>\n",
       "      <td>11260.200000</td>\n",
       "      <td>-4.758150</td>\n",
       "      <td>-3.135630</td>\n",
       "      <td>1.380860e+04</td>\n",
       "      <td>10973.300000</td>\n",
       "      <td>-4.606330</td>\n",
       "      <td>-3.132610</td>\n",
       "      <td>1.119760e+04</td>\n",
       "      <td>10067.900000</td>\n",
       "      <td>-4.814380</td>\n",
       "      <td>-3.136380</td>\n",
       "      <td>1.615530e+04</td>\n",
       "      <td>10183.700000</td>\n",
       "      <td>-4.803880</td>\n",
       "      <td>-3.135910</td>\n",
       "      <td>2.004750e+04</td>\n",
       "      <td>14800.200000</td>\n",
       "      <td>-4.400470</td>\n",
       "      <td>-3.130690</td>\n",
       "      <td>1.780380e+04</td>\n",
       "      <td>12987.900000</td>\n",
       "      <td>-4.447660</td>\n",
       "      <td>-3.139820</td>\n",
       "      <td>2.512510e+04</td>\n",
       "      <td>14836.000000</td>\n",
       "      <td>-4.448760</td>\n",
       "      <td>-2.990730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1249.750000</td>\n",
       "      <td>24352.375000</td>\n",
       "      <td>-1.619905</td>\n",
       "      <td>1.369522e+05</td>\n",
       "      <td>8.883690e+04</td>\n",
       "      <td>-1.035570</td>\n",
       "      <td>-1.574213</td>\n",
       "      <td>1.007510e+05</td>\n",
       "      <td>6.321840e+04</td>\n",
       "      <td>-1.060500</td>\n",
       "      <td>-1.602460</td>\n",
       "      <td>7.636905e+04</td>\n",
       "      <td>46549.475000</td>\n",
       "      <td>-1.125620</td>\n",
       "      <td>-1.547418</td>\n",
       "      <td>5.999090e+04</td>\n",
       "      <td>36097.700000</td>\n",
       "      <td>-1.121240</td>\n",
       "      <td>-1.518030</td>\n",
       "      <td>5.278370e+04</td>\n",
       "      <td>30891.650000</td>\n",
       "      <td>-1.198468</td>\n",
       "      <td>-1.550615</td>\n",
       "      <td>5.007050e+04</td>\n",
       "      <td>28453.95000</td>\n",
       "      <td>-1.250050</td>\n",
       "      <td>-1.586675</td>\n",
       "      <td>4.695560e+04</td>\n",
       "      <td>27963.500000</td>\n",
       "      <td>-1.231420</td>\n",
       "      <td>-1.475380</td>\n",
       "      <td>4.535515e+04</td>\n",
       "      <td>27140.550000</td>\n",
       "      <td>-1.243962</td>\n",
       "      <td>-1.626688</td>\n",
       "      <td>4.387110e+04</td>\n",
       "      <td>26825.000000</td>\n",
       "      <td>-1.226980</td>\n",
       "      <td>-1.513330</td>\n",
       "      <td>4.410735e+04</td>\n",
       "      <td>26589.250000</td>\n",
       "      <td>-1.223240</td>\n",
       "      <td>-1.422415</td>\n",
       "      <td>4.092160e+04</td>\n",
       "      <td>25298.300000</td>\n",
       "      <td>-1.413650</td>\n",
       "      <td>-1.270700</td>\n",
       "      <td>4.365005e+04</td>\n",
       "      <td>24742.500000</td>\n",
       "      <td>-1.259230</td>\n",
       "      <td>-1.817600</td>\n",
       "      <td>4.112588e+04</td>\n",
       "      <td>24974.125000</td>\n",
       "      <td>-1.243362</td>\n",
       "      <td>-1.490900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2499.500000</td>\n",
       "      <td>46814.400000</td>\n",
       "      <td>-0.055612</td>\n",
       "      <td>2.263525e+05</td>\n",
       "      <td>1.182015e+05</td>\n",
       "      <td>-0.038731</td>\n",
       "      <td>-0.009037</td>\n",
       "      <td>1.659740e+05</td>\n",
       "      <td>8.584360e+04</td>\n",
       "      <td>-0.057428</td>\n",
       "      <td>0.015111</td>\n",
       "      <td>1.288565e+05</td>\n",
       "      <td>62498.400000</td>\n",
       "      <td>-0.040648</td>\n",
       "      <td>0.034238</td>\n",
       "      <td>9.922610e+04</td>\n",
       "      <td>48949.200000</td>\n",
       "      <td>-0.035512</td>\n",
       "      <td>0.060279</td>\n",
       "      <td>9.206885e+04</td>\n",
       "      <td>41054.850000</td>\n",
       "      <td>0.054393</td>\n",
       "      <td>-0.079641</td>\n",
       "      <td>8.593460e+04</td>\n",
       "      <td>37378.30000</td>\n",
       "      <td>-0.046667</td>\n",
       "      <td>0.040528</td>\n",
       "      <td>7.975460e+04</td>\n",
       "      <td>34681.700000</td>\n",
       "      <td>0.025305</td>\n",
       "      <td>0.046141</td>\n",
       "      <td>8.315485e+04</td>\n",
       "      <td>33683.550000</td>\n",
       "      <td>0.156083</td>\n",
       "      <td>-0.015617</td>\n",
       "      <td>7.894980e+04</td>\n",
       "      <td>33328.000000</td>\n",
       "      <td>0.072709</td>\n",
       "      <td>-0.052590</td>\n",
       "      <td>7.609735e+04</td>\n",
       "      <td>30942.700000</td>\n",
       "      <td>0.035675</td>\n",
       "      <td>0.090282</td>\n",
       "      <td>7.568430e+04</td>\n",
       "      <td>29479.700000</td>\n",
       "      <td>-0.088908</td>\n",
       "      <td>-0.041002</td>\n",
       "      <td>8.050910e+04</td>\n",
       "      <td>28262.800000</td>\n",
       "      <td>0.120301</td>\n",
       "      <td>-0.232455</td>\n",
       "      <td>9.553645e+04</td>\n",
       "      <td>27353.550000</td>\n",
       "      <td>-0.121213</td>\n",
       "      <td>0.128103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3749.250000</td>\n",
       "      <td>83032.350000</td>\n",
       "      <td>1.537323</td>\n",
       "      <td>4.077158e+05</td>\n",
       "      <td>1.771265e+05</td>\n",
       "      <td>0.943598</td>\n",
       "      <td>1.542370</td>\n",
       "      <td>2.999950e+05</td>\n",
       "      <td>1.238700e+05</td>\n",
       "      <td>1.028340</td>\n",
       "      <td>1.605210</td>\n",
       "      <td>2.421225e+05</td>\n",
       "      <td>89587.500000</td>\n",
       "      <td>1.066302</td>\n",
       "      <td>1.570887</td>\n",
       "      <td>1.914340e+05</td>\n",
       "      <td>68782.100000</td>\n",
       "      <td>1.159480</td>\n",
       "      <td>1.612220</td>\n",
       "      <td>1.850510e+05</td>\n",
       "      <td>58596.225000</td>\n",
       "      <td>1.225223</td>\n",
       "      <td>1.508260</td>\n",
       "      <td>1.777845e+05</td>\n",
       "      <td>52731.10000</td>\n",
       "      <td>1.213660</td>\n",
       "      <td>1.587710</td>\n",
       "      <td>1.714090e+05</td>\n",
       "      <td>48486.200000</td>\n",
       "      <td>1.282950</td>\n",
       "      <td>1.479720</td>\n",
       "      <td>1.747630e+05</td>\n",
       "      <td>45464.775000</td>\n",
       "      <td>1.380947</td>\n",
       "      <td>1.541455</td>\n",
       "      <td>1.714670e+05</td>\n",
       "      <td>42325.500000</td>\n",
       "      <td>1.374130</td>\n",
       "      <td>1.394730</td>\n",
       "      <td>1.837982e+05</td>\n",
       "      <td>39158.225000</td>\n",
       "      <td>1.532125</td>\n",
       "      <td>1.565650</td>\n",
       "      <td>1.634690e+05</td>\n",
       "      <td>36154.100000</td>\n",
       "      <td>1.416310</td>\n",
       "      <td>1.514030</td>\n",
       "      <td>1.578350e+05</td>\n",
       "      <td>35445.700000</td>\n",
       "      <td>1.727295</td>\n",
       "      <td>1.712720</td>\n",
       "      <td>1.754910e+05</td>\n",
       "      <td>33817.950000</td>\n",
       "      <td>1.800682</td>\n",
       "      <td>1.984745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4999.000000</td>\n",
       "      <td>692674.000000</td>\n",
       "      <td>3.141130</td>\n",
       "      <td>3.186360e+06</td>\n",
       "      <td>1.276710e+06</td>\n",
       "      <td>4.141410</td>\n",
       "      <td>3.138540</td>\n",
       "      <td>3.587700e+06</td>\n",
       "      <td>1.146330e+06</td>\n",
       "      <td>4.559150</td>\n",
       "      <td>3.139200</td>\n",
       "      <td>2.800410e+06</td>\n",
       "      <td>788338.000000</td>\n",
       "      <td>4.798090</td>\n",
       "      <td>3.139020</td>\n",
       "      <td>2.503590e+06</td>\n",
       "      <td>481884.000000</td>\n",
       "      <td>4.730480</td>\n",
       "      <td>3.139660</td>\n",
       "      <td>3.039490e+06</td>\n",
       "      <td>331592.000000</td>\n",
       "      <td>4.882960</td>\n",
       "      <td>3.139910</td>\n",
       "      <td>2.791060e+06</td>\n",
       "      <td>452162.00000</td>\n",
       "      <td>4.760630</td>\n",
       "      <td>3.140370</td>\n",
       "      <td>1.981390e+06</td>\n",
       "      <td>293028.000000</td>\n",
       "      <td>4.781060</td>\n",
       "      <td>3.131490</td>\n",
       "      <td>1.932880e+06</td>\n",
       "      <td>524284.000000</td>\n",
       "      <td>4.802970</td>\n",
       "      <td>3.133340</td>\n",
       "      <td>2.111250e+06</td>\n",
       "      <td>341431.000000</td>\n",
       "      <td>4.372340</td>\n",
       "      <td>3.137560</td>\n",
       "      <td>1.529210e+06</td>\n",
       "      <td>369567.000000</td>\n",
       "      <td>4.592710</td>\n",
       "      <td>3.138930</td>\n",
       "      <td>2.073960e+06</td>\n",
       "      <td>220722.000000</td>\n",
       "      <td>4.790720</td>\n",
       "      <td>3.120760</td>\n",
       "      <td>1.246080e+06</td>\n",
       "      <td>167840.000000</td>\n",
       "      <td>4.691500</td>\n",
       "      <td>3.091510</td>\n",
       "      <td>1.177730e+06</td>\n",
       "      <td>155888.000000</td>\n",
       "      <td>4.151320</td>\n",
       "      <td>3.058890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID            MET       METphi            P1            P2  \\\n",
       "count  5000.000000    5000.000000  5000.000000  5.000000e+03  5.000000e+03   \n",
       "mean   2499.500000   64071.074332    -0.028916  3.301357e+05  1.540486e+05   \n",
       "std    1443.520003   60525.122480     1.819257  3.068202e+05  1.149469e+05   \n",
       "min       0.000000     290.756000    -3.141010  3.857940e+04  2.825400e+04   \n",
       "25%    1249.750000   24352.375000    -1.619905  1.369522e+05  8.883690e+04   \n",
       "50%    2499.500000   46814.400000    -0.055612  2.263525e+05  1.182015e+05   \n",
       "75%    3749.250000   83032.350000     1.537323  4.077158e+05  1.771265e+05   \n",
       "max    4999.000000  692674.000000     3.141130  3.186360e+06  1.276710e+06   \n",
       "\n",
       "                P3           P4            P5            P6           P7  \\\n",
       "count  5000.000000  5000.000000  4.997000e+03  4.997000e+03  4997.000000   \n",
       "mean     -0.039812    -0.003049  2.527799e+05  1.080302e+05    -0.029936   \n",
       "std       1.361762     1.814855  2.638580e+05  8.136261e+04     1.439105   \n",
       "min      -4.110220    -3.140710  1.087540e+04  1.080000e+04    -4.668790   \n",
       "25%      -1.035570    -1.574213  1.007510e+05  6.321840e+04    -1.060500   \n",
       "50%      -0.038731    -0.009037  1.659740e+05  8.584360e+04    -0.057428   \n",
       "75%       0.943598     1.542370  2.999950e+05  1.238700e+05     1.028340   \n",
       "max       4.141410     3.138540  3.587700e+06  1.146330e+06     4.559150   \n",
       "\n",
       "                P8            P9            P10          P11          P12  \\\n",
       "count  4997.000000  4.950000e+03    4950.000000  4950.000000  4950.000000   \n",
       "mean      0.007327  2.117980e+05   74863.343131    -0.025104     0.011845   \n",
       "std       1.828832  2.510361e+05   46309.512365     1.577316     1.802715   \n",
       "min      -3.140530  1.221050e+04   10639.800000    -4.520250    -3.141480   \n",
       "25%      -1.602460  7.636905e+04   46549.475000    -1.125620    -1.547418   \n",
       "50%       0.015111  1.288565e+05   62498.400000    -0.040648     0.034238   \n",
       "75%       1.605210  2.421225e+05   89587.500000     1.066302     1.570887   \n",
       "max       3.139200  2.800410e+06  788338.000000     4.798090     3.139020   \n",
       "\n",
       "                P13            P14          P15          P16           P17  \\\n",
       "count  4.717000e+03    4717.000000  4717.000000  4717.000000  4.002000e+03   \n",
       "mean   1.805997e+05   57289.049481     0.010723     0.045266  1.780366e+05   \n",
       "std    2.383403e+05   32013.857623     1.634072     1.812078  2.577958e+05   \n",
       "min    1.169190e+04   10818.000000    -4.616550    -3.136130  1.110310e+04   \n",
       "25%    5.999090e+04   36097.700000    -1.121240    -1.518030  5.278370e+04   \n",
       "50%    9.922610e+04   48949.200000    -0.035512     0.060279  9.206885e+04   \n",
       "75%    1.914340e+05   68782.100000     1.159480     1.612220  1.850510e+05   \n",
       "max    2.503590e+06  481884.000000     4.730480     3.139660  3.039490e+06   \n",
       "\n",
       "                 P18          P19          P20           P21           P22  \\\n",
       "count    4002.000000  4002.000000  4002.000000  2.871000e+03    2871.00000   \n",
       "mean    48798.018516     0.015167    -0.031312  1.705620e+05   44042.67015   \n",
       "std     26252.978520     1.744489     1.784248  2.381745e+05   23510.65367   \n",
       "min     10287.000000    -4.778980    -3.139040  1.070330e+04   10066.90000   \n",
       "25%     30891.650000    -1.198468    -1.550615  5.007050e+04   28453.95000   \n",
       "50%     41054.850000     0.054393    -0.079641  8.593460e+04   37378.30000   \n",
       "75%     58596.225000     1.225223     1.508260  1.777845e+05   52731.10000   \n",
       "max    331592.000000     4.882960     3.139910  2.791060e+06  452162.00000   \n",
       "\n",
       "               P23          P24           P25            P26          P27  \\\n",
       "count  2871.000000  2871.000000  1.889000e+03    1889.000000  1889.000000   \n",
       "mean     -0.022948     0.014522  1.628825e+05   41151.069666     0.002228   \n",
       "std       1.806611     1.811101  2.269341e+05   20988.953157     1.815312   \n",
       "min      -4.930230    -3.140380  1.197700e+04   11260.200000    -4.758150   \n",
       "25%      -1.250050    -1.586675  4.695560e+04   27963.500000    -1.231420   \n",
       "50%      -0.046667     0.040528  7.975460e+04   34681.700000     0.025305   \n",
       "75%       1.213660     1.587710  1.714090e+05   48486.200000     1.282950   \n",
       "max       4.760630     3.140370  1.981390e+06  293028.000000     4.781060   \n",
       "\n",
       "               P28           P29            P30          P31          P32  \\\n",
       "count  1889.000000  1.186000e+03    1186.000000  1186.000000  1186.000000   \n",
       "mean      0.006738  1.581409e+05   40250.387015     0.072349    -0.035907   \n",
       "std       1.771888  2.118782e+05   26556.025657     1.836492     1.796932   \n",
       "min      -3.135630  1.380860e+04   10973.300000    -4.606330    -3.132610   \n",
       "25%      -1.475380  4.535515e+04   27140.550000    -1.243962    -1.626688   \n",
       "50%       0.046141  8.315485e+04   33683.550000     0.156083    -0.015617   \n",
       "75%       1.479720  1.747630e+05   45464.775000     1.380947     1.541455   \n",
       "max       3.131490  1.932880e+06  524284.000000     4.802970     3.133340   \n",
       "\n",
       "                P33            P34         P35         P36           P37  \\\n",
       "count  7.290000e+02     729.000000  729.000000  729.000000  4.420000e+02   \n",
       "mean   1.596814e+05   40139.289849    0.061654   -0.045868  1.574039e+05   \n",
       "std    2.308620e+05   30074.756789    1.842798    1.788596  2.165489e+05   \n",
       "min    1.119760e+04   10067.900000   -4.814380   -3.136380  1.615530e+04   \n",
       "25%    4.387110e+04   26825.000000   -1.226980   -1.513330  4.410735e+04   \n",
       "50%    7.894980e+04   33328.000000    0.072709   -0.052590  7.609735e+04   \n",
       "75%    1.714670e+05   42325.500000    1.374130    1.394730  1.837982e+05   \n",
       "max    2.111250e+06  341431.000000    4.372340    3.137560  1.529210e+06   \n",
       "\n",
       "                 P38         P39         P40           P41            P42  \\\n",
       "count     442.000000  442.000000  442.000000  2.610000e+02     261.000000   \n",
       "mean    39703.038235    0.118543    0.024249  1.561160e+05   38173.716092   \n",
       "std     30502.312276    1.872084    1.826435  2.319016e+05   29324.658352   \n",
       "min     10183.700000   -4.803880   -3.135910  2.004750e+04   14800.200000   \n",
       "25%     26589.250000   -1.223240   -1.422415  4.092160e+04   25298.300000   \n",
       "50%     30942.700000    0.035675    0.090282  7.568430e+04   29479.700000   \n",
       "75%     39158.225000    1.532125    1.565650  1.634690e+05   36154.100000   \n",
       "max    369567.000000    4.592710    3.138930  2.073960e+06  220722.000000   \n",
       "\n",
       "              P43         P44           P45            P46         P47  \\\n",
       "count  261.000000  261.000000  1.270000e+02     127.000000  127.000000   \n",
       "mean     0.029455    0.026422  1.631051e+05   34876.849606    0.206978   \n",
       "std      1.884750    1.753017  2.248603e+05   20433.767238    1.998859   \n",
       "min     -4.400470   -3.130690  1.780380e+04   12987.900000   -4.447660   \n",
       "25%     -1.413650   -1.270700  4.365005e+04   24742.500000   -1.259230   \n",
       "50%     -0.088908   -0.041002  8.050910e+04   28262.800000    0.120301   \n",
       "75%      1.416310    1.514030  1.578350e+05   35445.700000    1.727295   \n",
       "max      4.790720    3.120760  1.246080e+06  167840.000000    4.691500   \n",
       "\n",
       "              P48           P49            P50        P51        P52  \n",
       "count  127.000000  5.600000e+01      56.000000  56.000000  56.000000  \n",
       "mean    -0.001085  1.456600e+05   36151.183929  -0.000879   0.219260  \n",
       "std      1.949004  1.943657e+05   25861.883410   1.941707   1.910400  \n",
       "min     -3.139820  2.512510e+04   14836.000000  -4.448760  -2.990730  \n",
       "25%     -1.817600  4.112588e+04   24974.125000  -1.243362  -1.490900  \n",
       "50%     -0.232455  9.553645e+04   27353.550000  -0.121213   0.128103  \n",
       "75%      1.712720  1.754910e+05   33817.950000   1.800682   1.984745  \n",
       "max      3.091510  1.177730e+06  155888.000000   4.151320   3.058890  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoQXel9US_Gy"
   },
   "source": [
    "Therefore, we will consider a subset of the data, so we have limited imputing/manipulation problems. Define \"features_lim\" as the new limited data set: it only contains the columns \n",
    "\n",
    "'MET', 'METphi', 'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11',\n",
    "       'P12',  'P13', 'P14', 'P15', and 'P16'\n",
    "       \n",
    "There may still be some columns with NaN values, so replace NaN with 0 for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p_L-a_R5S_Gy"
   },
   "outputs": [],
   "source": [
    "features_lim = features[['MET', 'METphi', 'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 'P15','P16']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lim = features_lim.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** replacing NaN with 0 is the simplest but worst possible choice - imputing a constant value skews the model. One step up would be to input the mean or median for each column. However, because only a limited number of instances have missing data, the choice of imputing strategy doesn't matter too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bGVvOk0S_Gz"
   },
   "source": [
    "Use \"describe()\" to confirm that the count is the same for all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vbnrvY0rS_G0",
    "outputId": "87922e80-d595-4dc9-da1b-34927e2ef184"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MET</th>\n",
       "      <th>METphi</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>P9</th>\n",
       "      <th>P10</th>\n",
       "      <th>P11</th>\n",
       "      <th>P12</th>\n",
       "      <th>P13</th>\n",
       "      <th>P14</th>\n",
       "      <th>P15</th>\n",
       "      <th>P16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>64071.074332</td>\n",
       "      <td>-0.028916</td>\n",
       "      <td>3.301357e+05</td>\n",
       "      <td>1.540486e+05</td>\n",
       "      <td>-0.039812</td>\n",
       "      <td>-0.003049</td>\n",
       "      <td>2.526283e+05</td>\n",
       "      <td>1.079653e+05</td>\n",
       "      <td>-0.029918</td>\n",
       "      <td>0.007323</td>\n",
       "      <td>2.096800e+05</td>\n",
       "      <td>74114.709700</td>\n",
       "      <td>-0.024853</td>\n",
       "      <td>0.011727</td>\n",
       "      <td>1.703778e+05</td>\n",
       "      <td>54046.489280</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>0.042704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>60525.122480</td>\n",
       "      <td>1.819257</td>\n",
       "      <td>3.068202e+05</td>\n",
       "      <td>1.149469e+05</td>\n",
       "      <td>1.361762</td>\n",
       "      <td>1.814855</td>\n",
       "      <td>2.638514e+05</td>\n",
       "      <td>8.138121e+04</td>\n",
       "      <td>1.438673</td>\n",
       "      <td>1.828283</td>\n",
       "      <td>2.506651e+05</td>\n",
       "      <td>46675.655162</td>\n",
       "      <td>1.569410</td>\n",
       "      <td>1.793678</td>\n",
       "      <td>2.352279e+05</td>\n",
       "      <td>33795.723384</td>\n",
       "      <td>1.587146</td>\n",
       "      <td>1.760070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>290.756000</td>\n",
       "      <td>-3.141010</td>\n",
       "      <td>3.857940e+04</td>\n",
       "      <td>2.825400e+04</td>\n",
       "      <td>-4.110220</td>\n",
       "      <td>-3.140710</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.668790</td>\n",
       "      <td>-3.140530</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.520250</td>\n",
       "      <td>-3.141480</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.616550</td>\n",
       "      <td>-3.136130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24352.375000</td>\n",
       "      <td>-1.619905</td>\n",
       "      <td>1.369522e+05</td>\n",
       "      <td>8.883690e+04</td>\n",
       "      <td>-1.035570</td>\n",
       "      <td>-1.574213</td>\n",
       "      <td>1.007050e+05</td>\n",
       "      <td>6.320943e+04</td>\n",
       "      <td>-1.059270</td>\n",
       "      <td>-1.599617</td>\n",
       "      <td>7.488228e+04</td>\n",
       "      <td>46165.375000</td>\n",
       "      <td>-1.108390</td>\n",
       "      <td>-1.532478</td>\n",
       "      <td>5.480870e+04</td>\n",
       "      <td>33959.400000</td>\n",
       "      <td>-1.050477</td>\n",
       "      <td>-1.424080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>46814.400000</td>\n",
       "      <td>-0.055612</td>\n",
       "      <td>2.263525e+05</td>\n",
       "      <td>1.182015e+05</td>\n",
       "      <td>-0.038731</td>\n",
       "      <td>-0.009037</td>\n",
       "      <td>1.658985e+05</td>\n",
       "      <td>8.581595e+04</td>\n",
       "      <td>-0.056810</td>\n",
       "      <td>0.012737</td>\n",
       "      <td>1.277135e+05</td>\n",
       "      <td>62167.100000</td>\n",
       "      <td>-0.023321</td>\n",
       "      <td>0.006687</td>\n",
       "      <td>9.259335e+04</td>\n",
       "      <td>47278.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>83032.350000</td>\n",
       "      <td>1.537323</td>\n",
       "      <td>4.077158e+05</td>\n",
       "      <td>1.771265e+05</td>\n",
       "      <td>0.943598</td>\n",
       "      <td>1.542370</td>\n",
       "      <td>2.999058e+05</td>\n",
       "      <td>1.238520e+05</td>\n",
       "      <td>1.028055</td>\n",
       "      <td>1.601880</td>\n",
       "      <td>2.406498e+05</td>\n",
       "      <td>89065.300000</td>\n",
       "      <td>1.048617</td>\n",
       "      <td>1.553310</td>\n",
       "      <td>1.831228e+05</td>\n",
       "      <td>66846.300000</td>\n",
       "      <td>1.085627</td>\n",
       "      <td>1.521765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>692674.000000</td>\n",
       "      <td>3.141130</td>\n",
       "      <td>3.186360e+06</td>\n",
       "      <td>1.276710e+06</td>\n",
       "      <td>4.141410</td>\n",
       "      <td>3.138540</td>\n",
       "      <td>3.587700e+06</td>\n",
       "      <td>1.146330e+06</td>\n",
       "      <td>4.559150</td>\n",
       "      <td>3.139200</td>\n",
       "      <td>2.800410e+06</td>\n",
       "      <td>788338.000000</td>\n",
       "      <td>4.798090</td>\n",
       "      <td>3.139020</td>\n",
       "      <td>2.503590e+06</td>\n",
       "      <td>481884.000000</td>\n",
       "      <td>4.730480</td>\n",
       "      <td>3.139660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MET       METphi            P1            P2           P3  \\\n",
       "count    5000.000000  5000.000000  5.000000e+03  5.000000e+03  5000.000000   \n",
       "mean    64071.074332    -0.028916  3.301357e+05  1.540486e+05    -0.039812   \n",
       "std     60525.122480     1.819257  3.068202e+05  1.149469e+05     1.361762   \n",
       "min       290.756000    -3.141010  3.857940e+04  2.825400e+04    -4.110220   \n",
       "25%     24352.375000    -1.619905  1.369522e+05  8.883690e+04    -1.035570   \n",
       "50%     46814.400000    -0.055612  2.263525e+05  1.182015e+05    -0.038731   \n",
       "75%     83032.350000     1.537323  4.077158e+05  1.771265e+05     0.943598   \n",
       "max    692674.000000     3.141130  3.186360e+06  1.276710e+06     4.141410   \n",
       "\n",
       "                P4            P5            P6           P7           P8  \\\n",
       "count  5000.000000  5.000000e+03  5.000000e+03  5000.000000  5000.000000   \n",
       "mean     -0.003049  2.526283e+05  1.079653e+05    -0.029918     0.007323   \n",
       "std       1.814855  2.638514e+05  8.138121e+04     1.438673     1.828283   \n",
       "min      -3.140710  0.000000e+00  0.000000e+00    -4.668790    -3.140530   \n",
       "25%      -1.574213  1.007050e+05  6.320943e+04    -1.059270    -1.599617   \n",
       "50%      -0.009037  1.658985e+05  8.581595e+04    -0.056810     0.012737   \n",
       "75%       1.542370  2.999058e+05  1.238520e+05     1.028055     1.601880   \n",
       "max       3.138540  3.587700e+06  1.146330e+06     4.559150     3.139200   \n",
       "\n",
       "                 P9            P10          P11          P12           P13  \\\n",
       "count  5.000000e+03    5000.000000  5000.000000  5000.000000  5.000000e+03   \n",
       "mean   2.096800e+05   74114.709700    -0.024853     0.011727  1.703778e+05   \n",
       "std    2.506651e+05   46675.655162     1.569410     1.793678  2.352279e+05   \n",
       "min    0.000000e+00       0.000000    -4.520250    -3.141480  0.000000e+00   \n",
       "25%    7.488228e+04   46165.375000    -1.108390    -1.532478  5.480870e+04   \n",
       "50%    1.277135e+05   62167.100000    -0.023321     0.006687  9.259335e+04   \n",
       "75%    2.406498e+05   89065.300000     1.048617     1.553310  1.831228e+05   \n",
       "max    2.800410e+06  788338.000000     4.798090     3.139020  2.503590e+06   \n",
       "\n",
       "                 P14          P15          P16  \n",
       "count    5000.000000  5000.000000  5000.000000  \n",
       "mean    54046.489280     0.010116     0.042704  \n",
       "std     33795.723384     1.587146     1.760070  \n",
       "min         0.000000    -4.616550    -3.136130  \n",
       "25%     33959.400000    -1.050477    -1.424080  \n",
       "50%     47278.800000     0.000000     0.000000  \n",
       "75%     66846.300000     1.085627     1.521765  \n",
       "max    481884.000000     4.730480     3.139660  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_lim.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8EHnF1OS_G0"
   },
   "source": [
    "## Step 4: labels and benchmarking\n",
    "\n",
    "What percentage of the data has the negative label (0) versus the positive label (1)? What is the accuracy of a classifier that puts everything in the negative class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "UCtTNHcxS_G0",
    "outputId": "aafd29ac-f80d-40eb-8e50-8d4787aa1d3c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative label (0): 0.8378\n",
      "Positve label (1): 0.1622\n",
      "Accuracy of a classifier putting everything in the negative class: 0.8378\n"
     ]
    }
   ],
   "source": [
    "percentage_neg = (len(target) - np.sum(target))/len(target)\n",
    "percentage_pos = np.sum(target)/len(target)\n",
    "\n",
    "print(\"Negative label (0):\", percentage_neg)\n",
    "print(\"Positve label (1):\", percentage_pos)\n",
    "\n",
    "print(\"Accuracy of a classifier putting everything in the negative class:\",percentage_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fIqGwitS_G0"
   },
   "source": [
    "For contrast, a random classifier that just assigns a random value according to class distribution has the following accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b1s2iKUS_G0",
    "outputId": "84c52ae9-fa4b-4f68-a481-2dc7d8eed3b0"
   },
   "source": [
    "### - A random classifer would assign random values baised on the class distribution, so the accuracy would be the same of the percentage of the negative label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve69kWKjS_G1"
   },
   "source": [
    "## Step 5: Let's start with a linear model; model = LinearSVC()\n",
    "\n",
    "1. Define a benchmark linear model, \"bmodel\", using LinearSVC(dual=False).\n",
    "\n",
    "2. Use \"StratifiedKFold\" with 5 splits, shuffle set to True, and a random state of 101 to produce a cross-validation object, \"cv\".\n",
    "\n",
    "3. Run \"cross_validate\", where scoring = 'accuracy' and return_train_score=True. This will output the fit time, the score time, the test score, and the train score. Print these.\n",
    "\n",
    "4. Print the mean and standard deviation of the test_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ixh-JsVOS_G1"
   },
   "outputs": [],
   "source": [
    "bmodel = LinearSVC(dual=False)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pZpp047LS_G2",
    "outputId": "1f13b55d-6aeb-4bd9-f8f1-ad1a628d9542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Time: [0.23280621 0.0153234  0.01566195 0.01557994 0.02520585]\n",
      "Score Time: [0.03154564 0.         0.         0.         0.00120687]\n",
      "Test Score: [0.841 0.825 0.829 0.83  0.833]\n",
      "Train Score: [0.8315  0.83275 0.8315  0.83125 0.832  ]\n",
      "\n",
      "Mean Test Score: 0.8316000000000001\n",
      "Standard Deviation of Test Score: 0.005351635264103866\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(bmodel, features_lim, target, scoring='accuracy', cv=cv, return_train_score=True)\n",
    "\n",
    "print(\"Fit Time:\", scores['fit_time'])\n",
    "print(\"Score Time:\", scores['score_time'])\n",
    "print(\"Test Score:\", scores['test_score'])\n",
    "print(\"Train Score:\", scores['train_score'])\n",
    "\n",
    "mean_test_score = scores['test_score'].mean()\n",
    "std_test_score = scores['test_score'].std()\n",
    "\n",
    "print(\"\\nMean Test Score:\", mean_test_score)\n",
    "print(\"Standard Deviation of Test Score:\", std_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oBiGeXoS_G2"
   },
   "source": [
    "## Step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, standardizing/normalizing data using the entire learning set introduces leakage between train and test set (the test set \"knows\" about the mean and standard deviation of the entire data set). Usually this is not a dramatic effect, but the correct procedure is to derive the scaler within each CV fold (i.e. after separating in train and test), only on the train set, and apply the same transformation to the test set. The model then becomes a pipeline.\n",
    "\n",
    "Similar to lab 5-6, set up a pipeline with StandardScaler and LinearSVC(dual=False,C=1000), use it in cross_validate and report the results. For the test and train scores, print the mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9SaXQ-JkS_G2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Time: [0.00554895 0.00797772 0.00836325 0.00051498 0.01554894]\n",
      "Score Time: [0.00805259 0.0023644  0.00239611 0.         0.        ]\n",
      "Test Score: [0.894 0.889 0.89  0.892 0.899]\n",
      "Train Score: [0.89575 0.895   0.895   0.89825 0.89275]\n",
      "\n",
      "Mean Test Score: 0.8928\n",
      "Standard Deviation of Test Score: 0.0035440090293338733\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline([('scaler', StandardScaler()),('classifier', LinearSVC(dual=False, C=1000))])\n",
    "\n",
    "scores_norm = cross_validate(model, features_lim, target, scoring='accuracy', cv=cv, return_train_score=True)\n",
    "\n",
    "print(\"Fit Time:\", scores_norm['fit_time'])\n",
    "print(\"Score Time:\", scores_norm['score_time'])\n",
    "print(\"Test Score:\", scores_norm['test_score'])\n",
    "print(\"Train Score:\", scores_norm['train_score'])\n",
    "\n",
    "print(\"\\nMean Test Score:\", scores_norm['test_score'].mean())\n",
    "print(\"Standard Deviation of Test Score:\", scores_norm['test_score'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test and train scores of benchmark_lim_piped, print the mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kxC_wvRES_G3",
    "outputId": "1adce87f-995d-4f0c-e476-855bb759b1dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Score: 0.8928\n",
      "Standard Deviation of Test Score: 0.0035440090293338733\n",
      "\n",
      "Mean Train Score: 0.89535\n",
      "Standard Deviation of Train Score: 0.0017649362594722646\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Test Score:\", scores_norm['test_score'].mean())\n",
    "print(\"Standard Deviation of Test Score:\", scores_norm['test_score'].std())\n",
    "print(\"\\nMean Train Score:\", scores_norm['train_score'].mean())\n",
    "print(\"Standard Deviation of Train Score:\", scores_norm['train_score'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaaIzsRpS_G4"
   },
   "source": [
    "This should show a significant improvement, and the comparison between test and train scores tells us already something about the problem that we have. We can formalize this by looking at the learning curves, which tell us both about gap between train/test scores, AND whether we need more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDn2AoUOS_G4"
   },
   "source": [
    "## Step 7: Learning Curves\n",
    "\n",
    "As in lab 5-6, construct the learning curve (you can recycle code). What does it tell you? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It tells us how well this particular algorithm and hyperparamters suffer from a varience error or a bias error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ORDR8CfuS_HK"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(model, features_lim, target, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, label='Training Score', color='blue', marker='o')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.2)\n",
    "plt.plot(train_sizes, test_mean, label='Test Score', color='red', marker='o')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='red', alpha=0.2)\n",
    "\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2cIdAkWS_G4"
   },
   "source": [
    "Is there anything we can conclude from this graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can conclude for this particular graph that 1) adding more data would not help, 2) our model does not suffer from high varience, as there is a small difference between the training score and the test score. Any steps to improve would go towards solving high bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2SvyIshS_G5"
   },
   "source": [
    "## Step 8: Parameter optimization via cross-validation\n",
    "\n",
    "When we optimize parameters with a grid search, we choose the parameters that give the best test scores. This is different from what would happen with new data - to do this fairly, at no point of the training procedure we are allowed to look at the test labels. Therefore, we would need to do <b> nested cross validation </b> to avoid leakage between the parameter optimization and the cross validation procedure and properly evaluate the generalization error. For now, we are just looking for the best model so \"simple\" CV is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NNFoiueXS_G5",
    "outputId": "01da5aaf-c655-400b-e57b-0ee14a4066c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('standardscaler', StandardScaler()), ('svc', SVC())],\n",
       " 'verbose': False,\n",
       " 'standardscaler': StandardScaler(),\n",
       " 'svc': SVC(),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'svc__C': 1.0,\n",
       " 'svc__break_ties': False,\n",
       " 'svc__cache_size': 200,\n",
       " 'svc__class_weight': None,\n",
       " 'svc__coef0': 0.0,\n",
       " 'svc__decision_function_shape': 'ovr',\n",
       " 'svc__degree': 3,\n",
       " 'svc__gamma': 'scale',\n",
       " 'svc__kernel': 'rbf',\n",
       " 'svc__max_iter': -1,\n",
       " 'svc__probability': False,\n",
       " 'svc__random_state': None,\n",
       " 'svc__shrinking': True,\n",
       " 'svc__tol': 0.001,\n",
       " 'svc__verbose': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given:\n",
    "piped_model = make_pipeline(StandardScaler(), SVC()) #now using the general SVC so I can change the kernel\n",
    "piped_model.get_params() #this shows how we can access parameters both for the scaler and the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFPZcVTwS_G5"
   },
   "source": [
    "We can define a dictionary of parameter values to run the optimization. \n",
    "\n",
    "Note that this might take awhile (5-15 minutes); the early estimates output by this cell may be misleading because more complex models (in particular high gamma) take longer.\n",
    "\n",
    "Once you run this cell, the \"model\" object will have attributes \"best_score_\", \"best_params_\" and \"best_estimator_\", which give us access to the optimal estimator (printed out), as well as \"cv_results_\" that can be used to visualize the performance of all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iaR-DhChS_G5",
    "outputId": "eb8c3d83-200d-489c-dbb8-92aabb77eb43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best params, best score: 0.8964 {'svc__C': 1.0, 'svc__degree': 2, 'svc__gamma': 'scale', 'svc__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Given:\n",
    "%time\n",
    "#optimizing SVC: THIS IS NOT YET NESTED CV\n",
    "\n",
    "parameters = {'svc__kernel':['poly', 'rbf'], \\\n",
    "              'svc__gamma':['scale', 0.01, 0.1], 'svc__C':[0.1, 1.0, 10.0, 100.0], \\\n",
    "              'svc__degree': [2, 4]}\n",
    "\n",
    "model = GridSearchCV(piped_model, parameters, cv = StratifiedKFold(n_splits=5, shuffle=True), \\\n",
    "                     verbose = 4, n_jobs = -1, return_train_score=True)\n",
    "\n",
    "model.fit(features_lim,target)\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEfppjOGS_G6"
   },
   "source": [
    "## Step 9\n",
    "\n",
    "Next, we visualize the models in a pandas data frame, and rank them according to their test scores.\n",
    "\n",
    "You may find it useful to look at: \n",
    "1. the mean and std of the test scores\n",
    "2. the mean of the train scores (to evaluate if they differ and the significance of the result)\n",
    "3. fitting time (we can pick a faster model instead of the best model if the scores are comparable)!\n",
    "\n",
    "Let \"scores_lim\" be the dataframe containing \"model.cv_results_\". Print the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tY_72jBHS_G7",
    "outputId": "a1ba6846-051c-48b8-f917-97db27f9b393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of score_lim: Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
      "       'param_svc__C', 'param_svc__degree', 'param_svc__gamma',\n",
      "       'param_svc__kernel', 'params', 'split0_test_score', 'split1_test_score',\n",
      "       'split2_test_score', 'split3_test_score', 'split4_test_score',\n",
      "       'mean_test_score', 'std_test_score', 'rank_test_score',\n",
      "       'split0_train_score', 'split1_train_score', 'split2_train_score',\n",
      "       'split3_train_score', 'split4_train_score', 'mean_train_score',\n",
      "       'std_train_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "scores_lim = pd.DataFrame(model.cv_results_)\n",
    "print('Columns of score_lim:',scores_lim.columns )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in \"scores_lim\", sort the columns 'params','mean_test_score','std_test_score','mean_train_score', and 'mean_fit_time' by descending 'mean_test_score':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                               params  \\\n",
      "13     {'svc__C': 1.0, 'svc__degree': 2, 'svc__gamma': 'scale', 'svc__kernel': 'rbf'}   \n",
      "19     {'svc__C': 1.0, 'svc__degree': 4, 'svc__gamma': 'scale', 'svc__kernel': 'rbf'}   \n",
      "33       {'svc__C': 10.0, 'svc__degree': 4, 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}   \n",
      "27       {'svc__C': 10.0, 'svc__degree': 2, 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}   \n",
      "15        {'svc__C': 1.0, 'svc__degree': 2, 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}   \n",
      "21        {'svc__C': 1.0, 'svc__degree': 4, 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}   \n",
      "17         {'svc__C': 1.0, 'svc__degree': 2, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}   \n",
      "23         {'svc__C': 1.0, 'svc__degree': 4, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}   \n",
      "45      {'svc__C': 100.0, 'svc__degree': 4, 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}   \n",
      "39      {'svc__C': 100.0, 'svc__degree': 2, 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}   \n",
      "25    {'svc__C': 10.0, 'svc__degree': 2, 'svc__gamma': 'scale', 'svc__kernel': 'rbf'}   \n",
      "31    {'svc__C': 10.0, 'svc__degree': 4, 'svc__gamma': 'scale', 'svc__kernel': 'rbf'}   \n",
      "1      {'svc__C': 0.1, 'svc__degree': 2, 'svc__gamma': 'scale', 'svc__kernel': 'rbf'}   \n",
      "7      {'svc__C': 0.1, 'svc__degree': 4, 'svc__gamma': 'scale', 'svc__kernel': 'rbf'}   \n",
      "9         {'svc__C': 0.1, 'svc__degree': 4, 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}   \n",
      "3         {'svc__C': 0.1, 'svc__degree': 2, 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}   \n",
      "35        {'svc__C': 10.0, 'svc__degree': 4, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}   \n",
      "29        {'svc__C': 10.0, 'svc__degree': 2, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}   \n",
      "12    {'svc__C': 1.0, 'svc__degree': 2, 'svc__gamma': 'scale', 'svc__kernel': 'poly'}   \n",
      "28       {'svc__C': 10.0, 'svc__degree': 2, 'svc__gamma': 0.1, 'svc__kernel': 'poly'}   \n",
      "40      {'svc__C': 100.0, 'svc__degree': 2, 'svc__gamma': 0.1, 'svc__kernel': 'poly'}   \n",
      "36  {'svc__C': 100.0, 'svc__degree': 2, 'svc__gamma': 'scale', 'svc__kernel': 'poly'}   \n",
      "26      {'svc__C': 10.0, 'svc__degree': 2, 'svc__gamma': 0.01, 'svc__kernel': 'poly'}   \n",
      "24   {'svc__C': 10.0, 'svc__degree': 2, 'svc__gamma': 'scale', 'svc__kernel': 'poly'}   \n",
      "4         {'svc__C': 0.1, 'svc__degree': 2, 'svc__gamma': 0.1, 'svc__kernel': 'poly'}   \n",
      "18    {'svc__C': 1.0, 'svc__degree': 4, 'svc__gamma': 'scale', 'svc__kernel': 'poly'}   \n",
      "10        {'svc__C': 0.1, 'svc__degree': 4, 'svc__gamma': 0.1, 'svc__kernel': 'poly'}   \n",
      "16        {'svc__C': 1.0, 'svc__degree': 2, 'svc__gamma': 0.1, 'svc__kernel': 'poly'}   \n",
      "38     {'svc__C': 100.0, 'svc__degree': 2, 'svc__gamma': 0.01, 'svc__kernel': 'poly'}   \n",
      "22        {'svc__C': 1.0, 'svc__degree': 4, 'svc__gamma': 0.1, 'svc__kernel': 'poly'}   \n",
      "30   {'svc__C': 10.0, 'svc__degree': 4, 'svc__gamma': 'scale', 'svc__kernel': 'poly'}   \n",
      "0     {'svc__C': 0.1, 'svc__degree': 2, 'svc__gamma': 'scale', 'svc__kernel': 'poly'}   \n",
      "44     {'svc__C': 100.0, 'svc__degree': 4, 'svc__gamma': 0.01, 'svc__kernel': 'poly'}   \n",
      "6     {'svc__C': 0.1, 'svc__degree': 4, 'svc__gamma': 'scale', 'svc__kernel': 'poly'}   \n",
      "14       {'svc__C': 1.0, 'svc__degree': 2, 'svc__gamma': 0.01, 'svc__kernel': 'poly'}   \n",
      "41       {'svc__C': 100.0, 'svc__degree': 2, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}   \n",
      "47       {'svc__C': 100.0, 'svc__degree': 4, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}   \n",
      "37   {'svc__C': 100.0, 'svc__degree': 2, 'svc__gamma': 'scale', 'svc__kernel': 'rbf'}   \n",
      "43   {'svc__C': 100.0, 'svc__degree': 4, 'svc__gamma': 'scale', 'svc__kernel': 'rbf'}   \n",
      "11         {'svc__C': 0.1, 'svc__degree': 4, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}   \n",
      "5          {'svc__C': 0.1, 'svc__degree': 2, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}   \n",
      "34       {'svc__C': 10.0, 'svc__degree': 4, 'svc__gamma': 0.1, 'svc__kernel': 'poly'}   \n",
      "42  {'svc__C': 100.0, 'svc__degree': 4, 'svc__gamma': 'scale', 'svc__kernel': 'poly'}   \n",
      "32      {'svc__C': 10.0, 'svc__degree': 4, 'svc__gamma': 0.01, 'svc__kernel': 'poly'}   \n",
      "20       {'svc__C': 1.0, 'svc__degree': 4, 'svc__gamma': 0.01, 'svc__kernel': 'poly'}   \n",
      "46      {'svc__C': 100.0, 'svc__degree': 4, 'svc__gamma': 0.1, 'svc__kernel': 'poly'}   \n",
      "2        {'svc__C': 0.1, 'svc__degree': 2, 'svc__gamma': 0.01, 'svc__kernel': 'poly'}   \n",
      "8        {'svc__C': 0.1, 'svc__degree': 4, 'svc__gamma': 0.01, 'svc__kernel': 'poly'}   \n",
      "\n",
      "    mean_test_score  std_test_score  mean_train_score  mean_fit_time  \n",
      "13           0.8964        0.004543           0.92150       1.084525  \n",
      "19           0.8964        0.004543           0.92150       1.066120  \n",
      "33           0.8960        0.005060           0.91260       0.997140  \n",
      "27           0.8960        0.005060           0.91260       1.201323  \n",
      "15           0.8932        0.005036           0.90045       0.860476  \n",
      "21           0.8932        0.005036           0.90045       0.869499  \n",
      "17           0.8928        0.005913           0.93845       1.310376  \n",
      "23           0.8928        0.005913           0.93845       1.188346  \n",
      "45           0.8918        0.006675           0.93350       2.071391  \n",
      "39           0.8918        0.006675           0.93350       2.088798  \n",
      "25           0.8838        0.006997           0.96665       1.750509  \n",
      "31           0.8838        0.006997           0.96665       1.444741  \n",
      "1            0.8838        0.007111           0.88970       0.561615  \n",
      "7            0.8838        0.007111           0.88970       1.121331  \n",
      "9            0.8830        0.006481           0.88510       0.985260  \n",
      "3            0.8830        0.006481           0.88510       1.068010  \n",
      "35           0.8786        0.003007           0.99135       1.818134  \n",
      "29           0.8786        0.003007           0.99135       1.960456  \n",
      "12           0.8774        0.007499           0.88175       1.500573  \n",
      "28           0.8772        0.007833           0.88740       8.979961  \n",
      "40           0.8772        0.007833           0.88785      77.613178  \n",
      "36           0.8768        0.007440           0.88770      27.411352  \n",
      "26           0.8766        0.006468           0.87770       1.234210  \n",
      "24           0.8766        0.007499           0.88680       3.606306  \n",
      "4            0.8766        0.006468           0.87770       1.192467  \n",
      "18           0.8762        0.003655           0.91625       1.484885  \n",
      "10           0.8762        0.004214           0.91725       1.764037  \n",
      "16           0.8760        0.005933           0.88465       1.617069  \n",
      "38           0.8760        0.005933           0.88460       2.035393  \n",
      "22           0.8750        0.005692           0.96245       2.020530  \n",
      "30           0.8748        0.006242           0.96125       2.308499  \n",
      "0            0.8692        0.006615           0.87095       0.560489  \n",
      "44           0.8668        0.007250           0.88585       1.190982  \n",
      "6            0.8662        0.007574           0.88515       1.390217  \n",
      "14           0.8656        0.005643           0.86645       0.944629  \n",
      "41           0.8652        0.003124           1.00000       2.196754  \n",
      "47           0.8652        0.003124           1.00000       1.940460  \n",
      "37           0.8642        0.005845           0.99695       2.504212  \n",
      "43           0.8642        0.005845           0.99695       2.763550  \n",
      "11           0.8598        0.004261           0.86685       1.441600  \n",
      "5            0.8598        0.004261           0.86685       1.445683  \n",
      "34           0.8580        0.010640           0.99685       2.993923  \n",
      "42           0.8574        0.011960           0.99635       3.338886  \n",
      "32           0.8572        0.006274           0.86395       1.127137  \n",
      "20           0.8500        0.002966           0.85260       1.197810  \n",
      "46           0.8494        0.011689           1.00000       2.457792  \n",
      "2            0.8488        0.004261           0.84950       0.620177  \n",
      "8            0.8420        0.002683           0.84230       1.255241  \n"
     ]
    }
   ],
   "source": [
    "sorted_scores = scores_lim.sort_values(by='mean_test_score', ascending=False)[['params', 'mean_test_score', 'std_test_score', 'mean_train_score', 'mean_fit_time']]\n",
    "print(sorted_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build some intuition around the results, I find it helpful to ask: what hyperparameter values are common to all the best-performing models? Here, for example, the rbf kernel seems to be constantly preferred, while the values of C and gamma seem to only affect the scores only mildly. Note also that the Grid Search is insensitive to moot parameters combinations; for example, here the first three models are identical, because the degree of the polynomial kernel does not matter when using an rbf kernel. This is less than ideal, of course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiHw_EU-S_G7"
   },
   "source": [
    "### Final diagnosis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8lVS2-mS_HC"
   },
   "source": [
    "The problem here is high bias, which is not that surprising given that we are using only a subset of features.\n",
    "\n",
    "We can try two things: making up new features which might help, based on what we know about the problem, and using an imputing strategy to include information about the discarded features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prpkV4jgS_HC"
   },
   "source": [
    "### Step 10: Improve the model. \n",
    "\n",
    "In order to include additional features, use .fillna(0) to replace all nan with zero, and .replace('',0) to replace empty string values. Check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5_A5nbG0S_HC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MET    METphi        P1        P2        P3        P4        P5  \\\n",
      "0  62803.5 -1.810010  137571.0  128444.0 -0.345744 -0.307112  174209.0   \n",
      "1  57594.2 -0.509253  161529.0   80458.3 -1.318010  1.402050  291490.0   \n",
      "2  82313.3  1.686840  167130.0  113078.0  0.937258 -2.068680  102423.0   \n",
      "3  30610.8  2.617120  112267.0   61383.9 -1.211050 -1.457800   40647.8   \n",
      "4  45153.1 -2.241350  178174.0  100164.0  1.166880 -0.018721   92351.3   \n",
      "\n",
      "         P6        P7        P8        P9      P10       P11       P12  \\\n",
      "0  127932.0  0.826569  2.332000   86788.9  84554.9 -0.180795  2.187970   \n",
      "1   68462.9 -2.126740 -2.582310   44270.1  35139.6 -0.706120 -0.371392   \n",
      "2   54922.3  1.226850  0.646589   60768.9  36244.3  1.102890 -1.434480   \n",
      "3   39472.0 -0.024646 -2.222800  201589.0  32978.6 -2.496040  1.137810   \n",
      "4   69762.1  0.774114  2.568740   61625.2  50086.7  0.652572 -3.012800   \n",
      "\n",
      "        P13      P14      P15       P16  \n",
      "0  140289.0  76955.8 -1.19933 -1.302800  \n",
      "1   72883.9  26902.2 -1.65386 -3.129630  \n",
      "2   77714.0  27801.5  1.68461  1.389690  \n",
      "3   90096.7  26964.5  1.87132  0.817631  \n",
      "4  104193.0  31151.0  1.87641  0.865381  \n"
     ]
    }
   ],
   "source": [
    "features_lim = features_lim.fillna(0).replace('', 0)\n",
    "print(features_lim.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaAeaOZ8S_HD"
   },
   "source": [
    "#### Let's start by looking at what kind of particles we have as a product of the collision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yXFQVWRuS_HD",
    "outputId": "7172b92a-c670-4515-f675-c4bddd2cac34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['b', 'e+', 'e-', 'g', 'j', 'm+', 'm-', 'nan'], dtype='<U3')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array([features['Type_'+str(i)].values for i in range(1,14)]).astype('str'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIAKGgOqS_HE"
   },
   "source": [
    "#### Here are the proposed new features (justification can be found in Chapter 4).\n",
    "    \n",
    "    1. The total number of particles produced\n",
    "    2. The total number of b jets\n",
    "    3. The total number of jets\n",
    "    4. The total number of leptons (electrons, positron, mu+, mu-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "CBd2zGtuS_HE"
   },
   "outputs": [],
   "source": [
    "#count number of non-zero types \n",
    "ntot = np.array([-(np.sum(np.array([features['Type_'+str(i)].values[j] == 0 for i in range(1,14)])) - 13) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "R5TG2m0fS_HE"
   },
   "outputs": [],
   "source": [
    "#count number of b jets \n",
    "nbtot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'b' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "D3K5KSm4S_HE"
   },
   "outputs": [],
   "source": [
    "#Actually, let's count all types (jets, photons g, e-, e+, mu-, mu+)\n",
    "njtot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'j' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "rQhhVIvRS_HE"
   },
   "outputs": [],
   "source": [
    "ngtot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'g' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "5CLntzFrS_HE"
   },
   "outputs": [],
   "source": [
    "n_el_tot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'e-' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "upx72Ry0S_HE"
   },
   "outputs": [],
   "source": [
    "n_pos_tot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'e+' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7yL61wOTS_HF"
   },
   "outputs": [],
   "source": [
    "n_muneg_tot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'm-' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "DzgFacgzS_HF"
   },
   "outputs": [],
   "source": [
    "n_mupos_tot = np.array([np.sum(np.array([features['Type_'+str(i)].values[j] == 'm+' for i in range(1,14)])) for j in range(features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ygcSxcuNS_HF"
   },
   "outputs": [],
   "source": [
    "n_lepton_tot = n_el_tot + n_pos_tot + n_muneg_tot + n_mupos_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAdBDl-US_HF"
   },
   "source": [
    "Add these new features to the dataset and check the result. How many features are there now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "woyzSbSsS_HF"
   },
   "outputs": [],
   "source": [
    "features[\"ntot\"]=ntot\n",
    "features[\"nbtot\"]=nbtot\n",
    "features[\"njtot\"]=njtot\n",
    "features[\"ngtot\"]=ngtot\n",
    "features[\"n_lepton_tot\"]=n_lepton_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "mIC5oBplS_HF",
    "outputId": "83cdeb20-5f94-4590-882c-a755bc4e110d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 73\n"
     ]
    }
   ],
   "source": [
    "features.shape\n",
    "print(\"Number of features:\",features.shape[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QaMinaQS_HF"
   },
   "source": [
    "### Feature engineering: impact of ad-hoc variables\n",
    "\n",
    "Define 'features_lim_2' as the original 'features_lim' plus the new five hand-crafted features. Apply the piped model (StandardScaler plus LinearSVC) to this new dataset via cross_validate. Compute the mean and standard deviation of the test scores. Any improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "y55xL6SQS_HF"
   },
   "outputs": [],
   "source": [
    "new_features = features[[\"ntot\",\"nbtot\", \"njtot\", \"ngtot\",\"n_lepton_tot\"]]\n",
    "features_lim_2 = pd.concat([features_lim, new_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "9wHVYyKQS_HF",
    "outputId": "c1d03e50-e195-40ca-a59b-1dd74086d0d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Score: 0.9480000000000001\n",
      "Standard Deviation of Test Score: 0.008124038404635967\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline([('scaler', StandardScaler()),('classifier', LinearSVC(dual=False, C=1000))])\n",
    "\n",
    "scores_norm = cross_validate(model, features_lim_2, target, scoring='accuracy', cv=cv, return_train_score=True)\n",
    "\n",
    "print(\"Mean Test Score:\", scores_norm['test_score'].mean())\n",
    "print(\"Standard Deviation of Test Score:\", scores_norm['test_score'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajGNZOCvS_HF"
   },
   "source": [
    "## - There was an improvement in the test score and it was a lot quicker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Knowledge-informed feature engineering is often very successful, more than hyperparameter optimization. Machine learning methods are often tooted for their ability to learn relevant representations, but non-deep-learning methods are less capable to do so, and providing informative features is very helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4q395z_S_HG"
   },
   "source": [
    "We can optimize this model as well; it will take a while, just like the previous time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "znVTZHbsS_HG",
    "outputId": "8d32772b-f7b6-4603-fc1c-5f4aabb1e772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Âµs, sys: 7 Âµs, total: 10 Âµs\n",
      "Wall time: 6.91 Âµs\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best params, best score: 0.9446 {'svc__C': 1.0, 'svc__degree': 2, 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "#optimizing SVC: Takes a few minutes!\n",
    "piped_model = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "parameters = {'svc__kernel':['poly', 'rbf'], \\\n",
    "              'svc__gamma':['scale', 0.01, 0.1], 'svc__C':[0.1, 1.0, 10.0], 'svc__degree': [2, 4, 8]}\n",
    "\n",
    "nmodels = np.product([len(el) for el in parameters.values()])\n",
    "model = GridSearchCV(piped_model, parameters, cv = StratifiedKFold(n_splits=5, shuffle=True), \\\n",
    "                     verbose = 2, n_jobs = -1, return_train_score=True)\n",
    "model.fit(features_lim_2,target)\n",
    "\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-home message: feature engineering often works best if we use subject matter knowledge, and building more features is not necessarily better."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
