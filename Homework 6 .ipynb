{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02eaeb5",
   "metadata": {},
   "source": [
    "1. Prove formula (6.7) in the textbook, i.e. show that the variance of a sum of random but correlated variables can be written as $${\\rm Var}\\left(\\frac{1}{m}\\sum_{i=1}^m x_i\\right)= \\rho \\sigma^2 + \\frac{1}{m}(1-\\rho)\\sigma^2,$$\n",
    "where ${\\rm Var}(x_i)=\\sigma^2$ and the correlation coefficient $\\rho_{x_i,x_j}={\\rm Cov(x_i,x_j)}/\\sigma^2$\n",
    "\n",
    "(Hint: Review some properties of the covariance)\n",
    "\n",
    "You can write out the calculation by hand and attach a scanned pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d4b4fb",
   "metadata": {},
   "source": [
    "2. Load again the cleaned dataset from Lab 13 for the photometric redshift prediction with 6,307 objects and 6 features (sel_feature.csv and sel_target.csv). You can also just re-do the data cuts from the original file if you prefer.\n",
    "\n",
    "Optimize (using a Grid Search for the parameters you deem to be most relevant) the \tExtremely Random Tree algorithm and compute performance metric and the outlier fraction. How do they compare to the optimal Random Forest model? Comment not just on the \tscoring parameter(s), but also on high variance vs high bias. Which model would you pick?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f042314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, KFold, cross_val_predict, train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostRegressor, IsolationForest\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from astropy.io import fits\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14) \n",
    "matplotlib.rcParams.update({'figure.autolayout': False})\n",
    "matplotlib.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c61af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6307, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with fits.open('DEEP2_uniq_Terapix_Subaru_v1.fits') as data:\n",
    "    df = pd.DataFrame(np.array(data[1].data).byteswap().newbyteorder())\n",
    "df = df[df['zquality'] >= 3]\n",
    "df = df[df['cfhtls_source'] == 0]\n",
    "mags_columns = ['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor', 'y_apercor']\n",
    "df = df[(df[mags_columns] != -99).all(axis=1)]\n",
    "df = df[(df[mags_columns] != 99).all(axis=1)]\n",
    "final_features_df = df[mags_columns]\n",
    "final_target_df = df[['zhelio']]\n",
    "\n",
    "final_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c3e03fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extremely Random Tree:\n",
      "Mean Test Score: 0.7589850418354764\n",
      "Mean Train Score: 1.0\n",
      "Best Parameters: {'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Mean Absolute Error: 0.08034802976287642\n",
      "Outlier Fraction: 0.11986681465038845\n",
      "\n",
      "Random Forest:\n",
      "Mean Test Score: 0.7456736617631825\n",
      "Mean Train Score: 0.9617793809347829\n",
      "Mean Absolute Error: 0.08497041008311171\n",
      "Outlier Fraction: 0.12731885206912955\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(final_features_df,final_target_df,test_size=0.2,random_state=42)\n",
    "\n",
    "etr_model=ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'min_impurity_decrease': [0, 0.1, 0.5],\n",
    "    'max_leaf_nodes': [None, 100, 200],\n",
    "    'min_samples_split': [10, 20, 100],\n",
    "    'max_features': [None, 2, 4],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "grid_search=GridSearchCV(estimator=etr_model,param_grid=param_grid,scoring='neg_mean_absolute_error',cv=3,n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "best_params=grid_search.best_params_\n",
    "\n",
    "best_etr_model=ExtraTreesRegressor(random_state=42,**best_params)\n",
    "best_etr_model.fit(X_train,y_train.values.ravel())\n",
    "    \n",
    "cv_scores=cross_validate(best_etr_model,final_features_df,final_target_df.values.ravel(),cv=3,return_train_score=True)\n",
    "\n",
    "predictions=cross_val_predict(best_etr_model,final_features_df,final_target_df.values.ravel(),cv=3)\n",
    "mae=mean_absolute_error(final_target_df,predictions)\n",
    "outlier_fraction=np.sum(np.abs(predictions-final_target_df.values.ravel())>0.15)/len(predictions)\n",
    "\n",
    "print('Extremely Random Tree:')\n",
    "print('Mean Test Score:',np.mean(cv_scores['test_score']))\n",
    "print('Mean Train Score:',np.mean(cv_scores['train_score']))\n",
    "print('Best Parameters:',best_params)\n",
    "print('Mean Absolute Error:',mae)\n",
    "print('Outlier Fraction:',outlier_fraction)\n",
    "\n",
    "rf_model=RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search=GridSearchCV(estimator=rf_model,param_grid=param_grid,scoring='neg_mean_absolute_error',cv=3,n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "best_params=grid_search.best_params_\n",
    "\n",
    "best_rf_model=RandomForestRegressor(random_state=42,**best_params)\n",
    "best_rf_model.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "cv_scores=cross_validate(best_rf_model,final_features_df,final_target_df.values.ravel(),cv=3,return_train_score=True)\n",
    "\n",
    "predictions=cross_val_predict(best_rf_model,final_features_df,final_target_df.values.ravel(),cv=3)\n",
    "mae=mean_absolute_error(final_target_df,predictions)\n",
    "outlier_fraction=np.sum(np.abs(predictions-final_target_df.values.ravel())>0.15)/len(predictions)\n",
    "\n",
    "print('\\nRandom Forest:')\n",
    "print('Mean Test Score:',np.mean(cv_scores['test_score']))\n",
    "print('Mean Train Score:',np.mean(cv_scores['train_score']))\n",
    "print('Mean Absolute Error:',mae)\n",
    "print('Outlier Fraction:',outlier_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8955de1",
   "metadata": {},
   "source": [
    "#### -The Random Forest method has higher error and outlier fraction than the Extremely Random Tree. \n",
    "#### -The Extremely Random Tree performs approximately perfectly on the training data and not as well on the test data, indicating high variance, the scores are still high indicating low bias. Similar can be said for the Random Forest method, however the variance is slightly lower and the bias is slightly higher.\n",
    "#### -Since the Extremely Random Tree has lower mean absolute error and outlier fraction, and the test and train scores are similar to the Random Forest Method, I would choose the Extremely Random Tree Method. The ERT Method also seems to run faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
